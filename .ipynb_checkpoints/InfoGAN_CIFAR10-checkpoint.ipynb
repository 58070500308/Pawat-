{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "if not os.path.exists('out2'):\n",
    "    os.makedirs('out2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrontEnd(nn.Module):\n",
    "  ''' front end part of discriminator and Q'''\n",
    "  def __init__(self):\n",
    "    super(FrontEnd, self).__init__()\n",
    "    self.main = nn.Sequential(\n",
    "      nn.Conv2d(3, 64, 4, 2, 1),\n",
    "      nn.LeakyReLU(0.1, inplace=True),\n",
    "      nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "      nn.BatchNorm2d(128),\n",
    "      nn.LeakyReLU(0.1, inplace=True),\n",
    "      nn.Conv2d(128, 1024, 7, bias=False),\n",
    "      nn.BatchNorm2d(1024),\n",
    "      nn.LeakyReLU(0.1, inplace=True),)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x: torch.Size([100, 1, 32, 32])\n",
    "    output = self.main(x)\n",
    "    #print(\"Fe out\",output.shape)\n",
    "    return output\n",
    "\n",
    "class D(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(D, self).__init__()\n",
    "    self.main = nn.Sequential(\n",
    "      nn.Conv2d(1024, 1, 1),\n",
    "      nn.Sigmoid())\n",
    "\n",
    "  def forward(self, x):\n",
    "    output = self.main(x).view(-1, 1)\n",
    "    return output\n",
    "\n",
    "class Q(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(Q, self).__init__()\n",
    "    self.conv = nn.Conv2d(1024, 128, 1, bias=False)\n",
    "    self.bn = nn.BatchNorm2d(128)\n",
    "    self.lReLU = nn.LeakyReLU(0.1, inplace=True)\n",
    "    self.conv_disc = nn.Conv2d(128, 10, 1)\n",
    "    self.conv_mu = nn.Conv2d(128, 2, 1)\n",
    "    self.conv_var = nn.Conv2d(128, 2, 1)\n",
    "\n",
    "  def forward(self, x):\n",
    "    y = self.conv(x)\n",
    "    disc_logits = self.conv_disc(y).squeeze()\n",
    "\n",
    "    mu = self.conv_mu(y).squeeze()\n",
    "    var = self.conv_var(y).squeeze().exp()\n",
    "\n",
    "    return disc_logits, mu, var \n",
    "\n",
    "class G(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(G, self).__init__()\n",
    "    self.main = nn.Sequential(\n",
    "      nn.ConvTranspose2d(74, 1024, 1, 1, bias=False),\n",
    "      nn.BatchNorm2d(1024),\n",
    "      nn.ReLU(True),\n",
    "      nn.ConvTranspose2d(1024, 128, 7, 1, bias=False),\n",
    "      nn.BatchNorm2d(128),\n",
    "      nn.ReLU(True),\n",
    "      nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "      nn.BatchNorm2d(64),\n",
    "      nn.ReLU(True),\n",
    "      nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),\n",
    "      nn.Sigmoid())\n",
    "\n",
    "  def forward(self, x):\n",
    "    output = self.main(x)\n",
    "    return output\n",
    "\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class log_gaussian:\n",
    "  def __call__(self, x, mu, var):\n",
    "    logli = -0.5*(var.mul(2*np.pi)+1e-6).log() - \\\n",
    "            (x-mu).pow(2).div(var.mul(2.0)+1e-6)\n",
    "    return logli.sum(1).mean().mul(-1)\n",
    "\n",
    "class Trainer:\n",
    "  def __init__(self, G, FE, D, Q):\n",
    "    self.G = G\n",
    "    self.FE = FE\n",
    "    self.D = D\n",
    "    self.Q = Q\n",
    "    self.batch_size = 100\n",
    "\n",
    "  def _noise_sample(self, dis_c, con_c, noise, bs):\n",
    "    idx = np.random.randint(10, size=bs) # 隨機從數字0~9隨機給100個\n",
    "    c = np.zeros((bs, 10)) # 準備紀錄C的特性\n",
    "    c[range(bs),idx] = 1.0 # one-hot-encoder 上述idx\n",
    "    dis_c.data.copy_(torch.Tensor(c)) # 複製上述 one-hot-encoder\n",
    "    \n",
    "    con_c.data.uniform_(-1.0, 1.0)\n",
    "    noise.data.uniform_(-1.0, 1.0)\n",
    "    # noise.shape: torch.Size([100, 62]) # noise 62 個維度\n",
    "    # dis_c.shape: torch.Size([100, 10]) # 10個數字\n",
    "    # con_c.shape: torch.Size([100, 2]) # 2個特性\n",
    "    z = torch.cat([noise, dis_c, con_c], 1).view(-1, 74, 1, 1)\n",
    "    # z: torch.Size([100, 74, 1, 1])\n",
    "    return z, idx\n",
    "\n",
    "    # =================================== Start to train ==========================================\n",
    "  def train(self):\n",
    "    # define torch.size\n",
    "    real_x = torch.FloatTensor(self.batch_size, 3, 28, 28).cuda()#　torch.Size([100, 1, 28, 28])\n",
    "    label = torch.FloatTensor(self.batch_size, 1).cuda()# torch.Size([100, 1])\n",
    "    dis_c = torch.FloatTensor(self.batch_size, 10).cuda()# 分類數字 # torch.Size([100, 10])\n",
    "    con_c = torch.FloatTensor(self.batch_size, 2).cuda()# 分類特性 # # torch.Size([100, 2])\n",
    "    noise = torch.FloatTensor(self.batch_size, 62).cuda()# # torch.Size([100, 62])\n",
    "    \n",
    "    # 轉成torch格式\n",
    "    real_x = Variable(real_x)\n",
    "    label = Variable(label, requires_grad=False)\n",
    "    dis_c = Variable(dis_c)\n",
    "    con_c = Variable(con_c)\n",
    "    noise = Variable(noise)\n",
    "    \n",
    "    # define loss function\n",
    "    criterionD = nn.BCELoss().cuda()\n",
    "    criterionQ_dis = nn.CrossEntropyLoss().cuda()\n",
    "    criterionQ_con = log_gaussian()\n",
    "    \n",
    "    # define optimal function(Adam)\n",
    "    optimD = optim.Adam([{'params':self.FE.parameters()}, {'params':self.D.parameters()}], \n",
    "                        lr=0.0002, betas=(0.5, 0.99))\n",
    "    optimG = optim.Adam([{'params':self.G.parameters()}, {'params':self.Q.parameters()}],\n",
    "                        lr=0.001, betas=(0.5, 0.99))\n",
    "    \n",
    "    transform = transforms.Compose([transforms.Resize(28),transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    # load data\n",
    "    dataset = dset.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "    #dataset = dset.MNIST('./dataset', transform=transforms.ToTensor(), download=True) # 60000\n",
    "    dataloader = DataLoader(dataset, batch_size=100,shuffle=True, num_workers=2) # 600\n",
    "\n",
    "    # fixed random variables\n",
    "    # varying from -1 to 1(left to right).\n",
    "    c = np.linspace(-1, 1, 10).reshape(1, -1) # (Row,Cloumn)(1, 10)\n",
    "    c = np.repeat(c, 10, 0).reshape(-1, 1) # 重複10次 (100, 1)\n",
    "    \n",
    "    # (Width)\n",
    "    c1 = np.hstack([c, np.zeros_like(c)]) # (100, 2) # 第一行與 c一樣 第二行全部是0\n",
    "    # (Rotation)\n",
    "    c2 = np.hstack([np.zeros_like(c), c]) # (100, 2) # 第一行全部是0 第二行與 c一樣\n",
    "    \n",
    "    idx = np.arange(10).repeat(10) # 0~9每個數字重複10次\n",
    "    one_hot = np.zeros((100, 10)) # 定義一個都是0的陣列: (100, 10)\n",
    "    one_hot[range(100), idx] = 1 # one-hot-encoder 上述idx\n",
    "    \n",
    "    fix_noise = torch.Tensor(100, 62).uniform_(-1, 1) #　因batch size = 100, noise = 62, torch.Size([100, 62])\n",
    "    G_loss_app = []\n",
    "    D_loss_app = []\n",
    "    for epoch in range(300):\n",
    "      for num_iters, batch_data in enumerate(dataloader, 0):\n",
    "        # num_iters : 0~599\n",
    "        # ========================= Train Discriminator ================================\n",
    "        optimD.zero_grad()\n",
    "        \n",
    "        #　(real part)\n",
    "        x, _ = batch_data # x:真實數據 _: label\n",
    "        \n",
    "        bs = x.size(0) # 100\n",
    "        #print(bs)\n",
    "        #         real_x.data.resize_(x.size())\n",
    "        #         label.data.resize_(bs, 1)\n",
    "        #         dis_c.data.resize_(bs, 10)\n",
    "        #         con_c.data.resize_(bs, 2)\n",
    "        #         noise.data.resize_(bs, 62)\n",
    "        \n",
    "        real_x.data.copy_(x) # 真實數據 torch.Size([100, 1, 28, 28])\n",
    "    \n",
    "        fe_out1 = self.FE(real_x) # torch.Size([100, 1024, 1, 1])\n",
    "        #print(\"fe_out\",fe_out1.shape)\n",
    "        probs_real = self.D(fe_out1) # torch.Size([100, 1]) # Discriminator 辨識真實數據的結果\n",
    "        \n",
    "        label.data.fill_(1) # label = 1 # torch.Size([100, 1])\n",
    "        #print(\"label\",label.shape) #\n",
    "        #print(\"real prop\",probs_real.shape) #torch.Size([100, 1])\n",
    "        \n",
    "        loss_real = criterionD(probs_real, label) # 算Discriminator辨識 與 真實label 的loss\n",
    "        loss_real.backward() # 更新 算Discriminator辨識\n",
    "        \n",
    "        # (fake part)\n",
    "        '''\n",
    "            dis_c 分類數字, con_c 分類特性, noise 隨機亂數, bs = 100\n",
    "            z: torch.Size([100, 74, 1, 1])\n",
    "            idx: 隨機從數字0~9隨機給100個\n",
    "            noise.shape: torch.Size([100, 62]) # noise 62 個維度\n",
    "            dis_c.shape: torch.Size([100, 10]) # 10個數字\n",
    "            con_c.shape: torch.Size([100, 2]) # 2個特性\n",
    "        '''\n",
    "        z, idx = self._noise_sample(dis_c, con_c, noise, bs)\n",
    "        fake_x = self.G(z) # generate fake image\n",
    "        '''\n",
    "            當我們在訓練網絡的時候可能希望保持一部分的網絡參數不變，\n",
    "            只對其中一部分的參數進行調整；或者值訓練部分分支網絡，\n",
    "            並不讓其梯度對主網絡的梯度造成影響，\n",
    "            這時候我們就需要使用detach()函數來切斷一些分支的反向傳播\n",
    "            '''\n",
    "        fe_out2 = self.FE(fake_x.detach())\n",
    "        #print(\"feout2\",fe_out2.shape)\n",
    "        probs_fake = self.D(fe_out2)\n",
    "        #print(probs_fake.shape)\n",
    "        label.data.fill_(0)\n",
    "        loss_fake = criterionD(probs_fake, label)\n",
    "        loss_fake.backward()\n",
    "\n",
    "        D_loss = loss_real + loss_fake\n",
    "\n",
    "        optimD.step()\n",
    "        \n",
    "        # ========================= Train G ================================\n",
    "        # G and Q part\n",
    "        optimG.zero_grad()\n",
    "\n",
    "        fe_out = self.FE(fake_x)# fake_x: generate fake image\n",
    "        probs_fake = self.D(fe_out)\n",
    "        label.data.fill_(1.0)\n",
    "\n",
    "        reconstruct_loss = criterionD(probs_fake, label)\n",
    "        \n",
    "        # ========================= Train Q ================================\n",
    "        q_logits, q_mu, q_var = self.Q(fe_out)\n",
    "        class_ = torch.LongTensor(idx).cuda()\n",
    "        target = Variable(class_)\n",
    "        dis_loss = criterionQ_dis(q_logits, target)\n",
    "        con_loss = criterionQ_con(con_c, q_mu, q_var)*0.1\n",
    "        \n",
    "        G_loss = reconstruct_loss + dis_loss + con_loss\n",
    "        G_loss.backward()\n",
    "        optimG.step()\n",
    "    \n",
    "        if num_iters % 100 == 0 or num_iters == 599:\n",
    "          print('Epoch/Iter:{0}/{1}, Dloss: {2}, Gloss: {3}'.format(\n",
    "                    epoch, num_iters, D_loss.data.cpu().numpy(),\n",
    "                    G_loss.data.cpu().numpy()))          \n",
    "          noise.data.copy_(fix_noise) # 上述給定的隨機亂數\n",
    "          dis_c.data.copy_(torch.Tensor(one_hot)) #　上述給定的欲建構的數字\n",
    "\n",
    "          con_c.data.copy_(torch.from_numpy(c1))\n",
    "          z = torch.cat([noise, dis_c, con_c], 1).view(-1, 74, 1, 1) # 62+10+2 #torch.Size([100, 74, 1, 1])\n",
    "          x_save = self.G(z) # generate image\n",
    "          save_image(x_save.data, './out2/c1_%s_%s.png'%(epoch,num_iters), nrow=10)\n",
    "\n",
    "          con_c.data.copy_(torch.from_numpy(c2))\n",
    "          z = torch.cat([noise, dis_c, con_c], 1).view(-1, 74, 1, 1)\n",
    "          x_save = self.G(z) # generate image\n",
    "          save_image(x_save.data, './out2/c2_%s_%s.png'%(epoch,num_iters), nrow=10)\n",
    "        \n",
    "          if num_iters == 400 :         \n",
    "            D_loss_app.append(D_loss.data.cpu().numpy())      \n",
    "\n",
    "            G_loss_app.append(G_loss.data.cpu().numpy())\n",
    "        \n",
    "           \n",
    "    np.savetxt(\"./G_loss\",G_loss_app,delimiter=\",\")    \n",
    "    np.savetxt(\"./D_loss\",D_loss_app,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================ FrontEnd =================================\n",
      "FrontEnd(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (4): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (5): Conv2d(128, 1024, kernel_size=(7, 7), stride=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (7): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  )\n",
      ")\n",
      "================================ D =================================\n",
      "D(\n",
      "  (main): Sequential(\n",
      "    (0): Conv2d(1024, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (1): Sigmoid()\n",
      "  )\n",
      ")\n",
      "================================ Q =================================\n",
      "Q(\n",
      "  (conv): Conv2d(1024, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lReLU): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "  (conv_disc): Conv2d(128, 10, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv_mu): Conv2d(128, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv_var): Conv2d(128, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "================================ G =================================\n",
      "G(\n",
      "  (main): Sequential(\n",
      "    (0): ConvTranspose2d(74, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): ConvTranspose2d(1024, 128, kernel_size=(7, 7), stride=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "fe = FrontEnd() # discriminator and Q\n",
    "\n",
    "d = D()\n",
    "q = Q()\n",
    "g = G()\n",
    "\n",
    "print(\"================================ FrontEnd =================================\")\n",
    "print(fe)\n",
    "print(\"================================ D =================================\")\n",
    "print(d)\n",
    "print(\"================================ Q =================================\")\n",
    "print(q)\n",
    "print(\"================================ G =================================\")\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data\\cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55ca72f9ec2405ea7baf35ace9f2160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\cifar-10-python.tar.gz to ./data\n",
      "Epoch/Iter:0/0, Dloss: 1.428032398223877, Gloss: 3.259294033050537\n",
      "Epoch/Iter:0/100, Dloss: 1.0067318677902222, Gloss: 1.5836514234542847\n"
     ]
    }
   ],
   "source": [
    "# 初始化參數\n",
    "for i in [fe, d, q, g]:\n",
    "  i.cuda()\n",
    "  i.apply(weights_init)\n",
    "    \n",
    "trainer = Trainer(g, fe, d, q)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "torch.save(fe.state_dict(),'./mnist_infogan_fe_epoch100.pth')\n",
    "torch.save(d.state_dict(),'./mnist_infogan_d_epoch100.pth')\n",
    "torch.save(q.state_dict(),'./mnist_infogan_q_epoch100.pth')\n",
    "torch.save(g.state_dict(),'./mnist_infogan_G_epoch100.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlrd\n",
    "book = xlrd.open_workbook('100epochloss.xlsx')\n",
    "sheet1 = book.sheets()[1]\n",
    "D_values = sheet1.col_values(10)\n",
    "G_values = sheet1.col_values(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "G_losses = G_values\n",
    "D_losses = D_values\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "figurename='./epoch10_losscurve.png'\n",
    "plt.savefig(figurename,format='png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the fake image using different continue variable (c1) (Width)/continue variable (c2) (Rotation)\n",
    "\n",
    "def _noise_sample(dis_c, con_c, noise, bs):\n",
    "    idx = np.random.randint(10, size=bs) # 隨機從數字0~9隨機給100個\n",
    "    c = np.zeros((bs, 10)) # 準備紀錄C的特性\n",
    "    c[range(bs),idx] = 1.0 # one-hot-encoder 上述idx\n",
    "    dis_c.data.copy_(torch.Tensor(c)) # 複製上述 one-hot-encoder\n",
    "    \n",
    "    con_c.data.uniform_(-1.0, 1.0)\n",
    "    noise.data.uniform_(-1.0, 1.0)\n",
    "    z = torch.cat([noise, dis_c, con_c], 1).view(-1, 74, 1, 1)\n",
    "    return z, idx\n",
    "\n",
    "G = G()\n",
    "G_PATH = '/home/pawat/Desktop/Education/Home security/project/mnist_infogan_G_epoch100.pth'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    G.cuda()\n",
    "    pass\n",
    "\n",
    "G.load_state_dict(torch.load('mnist_infogan_G_epoch100.pth'))\n",
    "G.eval()\n",
    "\n",
    "bs_test = 100\n",
    "dis_c_test = torch.FloatTensor(100, 10).cuda()# 分類數字 # torch.Size([100, 10])\n",
    "con_c_test = torch.FloatTensor(100, 2).cuda()# 分類特性 # # torch.Size([100, 2])\n",
    "noise_test = torch.FloatTensor(100, 62).cuda()# # torch.Size([100, 62])\n",
    "dis_c_test = Variable(dis_c_test)\n",
    "con_c_test = Variable(con_c_test)\n",
    "noise_test = Variable(noise_test)\n",
    "\n",
    "fix_noise = torch.Tensor(100, 62).uniform_(-1, 1)\n",
    "noise_test.data.copy_(fix_noise)\n",
    "\n",
    "# fixed random variables # varying from -1 to 1(left to right).\n",
    "c = np.linspace(-1, -1, 10).reshape(1, -1) # (Row,Cloumn)(1, 10)\n",
    "c = np.repeat(c, 10, 0).reshape(-1, 1) # 重複10次 (100, 1)\n",
    "c1 = np.hstack([c, np.zeros_like(c)]) # (Width)\n",
    "c2 = np.hstack([np.zeros_like(c), c]) # (Rotation)\n",
    "    \n",
    "idx_test = np.arange(10).repeat(10) # 0~9每個數字重複10次\n",
    "print(idx_test)\n",
    "one_hot_test = np.zeros((100, 10)) # 定義一個都是0的陣列: (100, 10)\n",
    "one_hot_test[range(100), idx_test] = 1 # one-hot-encoder 上述idx\n",
    "dis_c_test.data.copy_(torch.Tensor(one_hot_test))\n",
    "\n",
    "# c1\n",
    "con_c_test.data.copy_(torch.from_numpy(c1))                                       \n",
    "z_test = torch.cat([noise_test, dis_c_test, con_c_test], 1).view(-1, 74, 1, 1) \n",
    "fake_x_test = G(z_test)\n",
    "img_path= './output_c1_test_-1.png'\n",
    "save_image(fake_x_test.data,img_path, nrow=10)\n",
    "\n",
    "\n",
    "# c2\n",
    "con_c_test.data.copy_(torch.from_numpy(c2))\n",
    "z_test = torch.cat([noise_test, dis_c_test, con_c_test], 1).view(-1, 74, 1, 1) \n",
    "fake_x_test = G(z_test)\n",
    "img_path= './output_c2_test_-1.png'\n",
    "save_image(fake_x_test.data,img_path, nrow=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
